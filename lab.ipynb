{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-05T00:10:37.425458Z",
     "start_time": "2025-12-05T00:10:37.411577Z"
    }
   },
   "source": [
    "from data import *\n",
    "from pretrained_models import *\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from mediapipe.tasks.python.vision.hand_landmarker import HandLandmarkerResult"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Sanity Check",
   "id": "aceed76a2b009de9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T03:46:51.336046Z",
     "start_time": "2025-12-05T03:46:51.314458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MediaPipeCFG = MediaPipeCfg(\"pretrained_model/hand_landmarker.task\")\n",
    "# options = MediaPipeCFG.create_options()\n",
    "# MP_model = MediaPipeCFG.HandLandmarker.create_from_options(options)"
   ],
   "id": "d6358d07f8fb7b45",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T03:46:51.486361Z",
     "start_time": "2025-12-05T03:46:51.474752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MMPoseCFG = MMPoseCfg(checkpoint_path='pretrained_model/checkpoint/rtmpose-s_simcc-body7_pt-body7_420e-256x192-acd4a1ef_20230504.pth',\n",
    "#                       config_path='pretrained_model/mmpose_config/rtmpose_m_8xb256-420e_coco-256x192.py')\n",
    "# body_model = MMPoseCFG.create_model()"
   ],
   "id": "2a4048478593bab3",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T03:46:51.734894Z",
     "start_time": "2025-12-05T03:46:51.718959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# PreTrainDataset = ASLData(video_dir=\"data/raw_videos\",\n",
    "#                            MP_model=MP_model,\n",
    "#                            body_cfg=MMPoseCFG,\n",
    "#                            body_model=body_model,\n",
    "#                            labels_path=\"data/how2sign_realigned_train.csv\")\n",
    "# def collate_as_is(batch):\n",
    "#     return batch\n",
    "# PreTrainDataLoader = DataLoader(PreTrainDataset, batch_size=4, shuffle=False, collate_fn=collate_as_is)"
   ],
   "id": "17db5e3ecba609bd",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T03:46:52.568022Z",
     "start_time": "2025-12-05T03:46:52.552402Z"
    }
   },
   "cell_type": "code",
   "source": "# OneSample = next(iter(PreTrainDataLoader))",
   "id": "29fbf0c787ba7902",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Four batches run in 2 m 45 secs",
   "id": "93bc1d6c0fced99c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Write converted JSON to local dir for training",
   "id": "99e231e4bc1e3690"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5bce19e916d2f068"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T20:21:01.173853Z",
     "start_time": "2025-12-02T20:21:01.157364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_dir = \"test_json_8_batches\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ],
   "id": "8826148f83d87ef2",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T20:38:12.306740Z",
     "start_time": "2025-12-02T20:24:19.501820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_json_safe(x):\n",
    "    \"\"\"\n",
    "    Recursively convert tensors, numpy arrays, and other non-serializable\n",
    "    objects into plain Python lists or scalars.\n",
    "    \"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.cpu().numpy().tolist()\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.tolist()\n",
    "    if isinstance(x, dict):\n",
    "        return {k: make_json_safe(v) for k, v in x.items()}\n",
    "    if isinstance(x, list):\n",
    "        return [make_json_safe(v) for v in x]\n",
    "    if isinstance(x, tuple):\n",
    "        return tuple(make_json_safe(v) for v in x)\n",
    "    return x  # scalar, string, or already json-safe\n",
    "\n",
    "print(\"Saving dataloader batches to:\", output_dir)\n",
    "\n",
    "for batch_idx, batch in enumerate(PreTrainDataLoader):\n",
    "    if batch_idx == 8:\n",
    "        break\n",
    "    json_safe_batch = make_json_safe(batch)\n",
    "\n",
    "    output_path = os.path.join(output_dir, f\"batch_{batch_idx:05d}.json\")\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_safe_batch, f, indent=2)\n",
    "\n",
    "    print(f\"[✓] Saved batch {batch_idx} → {output_path}\")\n",
    "\n",
    "\n",
    "print(\"Finished writing all batches!\")"
   ],
   "id": "a9f3796bddc43f72",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataloader batches to: test_json_8_batches\n",
      "[✓] Saved batch 0 → test_json_8_batches\\batch_00000.json\n",
      "[✓] Saved batch 1 → test_json_8_batches\\batch_00001.json\n",
      "[✓] Saved batch 2 → test_json_8_batches\\batch_00002.json\n",
      "[✓] Saved batch 3 → test_json_8_batches\\batch_00003.json\n",
      "[✓] Saved batch 4 → test_json_8_batches\\batch_00004.json\n",
      "[✓] Saved batch 5 → test_json_8_batches\\batch_00005.json\n",
      "[✓] Saved batch 6 → test_json_8_batches\\batch_00006.json\n",
      "[✓] Saved batch 7 → test_json_8_batches\\batch_00007.json\n",
      "Finished writing all batches!\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "8 batches runs in 13m 52s",
   "id": "1afb2375729c8367"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Read in json data into trainable data using Custom DataLoader and Text Tokenizer",
   "id": "96ad6111943884ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T21:32:11.284772Z",
     "start_time": "2025-12-02T21:32:08.901181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Build tokenizer & vocab from your saved JSONs\n",
    "tokenizer_fn, vocab, my_pad_id = create_tokenizer(\n",
    "    json_dir=\"test_json_8_batches\",  # or wherever you dumped them\n",
    "    min_freq=1                       # or >1 to prune rare tokens\n",
    ")\n",
    "\n",
    "print(\"PAD ID:\", my_pad_id)\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "\n",
    "# Create dataset\n",
    "dataset = ASLPoseJSONDataset(\n",
    "    json_dir=\"test_json_8_batches\",\n",
    "    tokenizer=tokenizer_fn,\n",
    "    max_frames=300,\n",
    "    frame_subsample=2,\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda b: asl_collate_fn(b, pad_id=my_pad_id),\n",
    ")\n"
   ],
   "id": "29e83141dfdd773a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAD ID: 0\n",
      "Vocab size: 239\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T21:32:12.386211Z",
     "start_time": "2025-12-02T21:32:11.302004Z"
    }
   },
   "cell_type": "code",
   "source": "next(iter(loader))",
   "id": "1d3f87862d4ee6fb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pose': tensor([[[1435.8334,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [1435.8334,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [1435.8334,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          ...,\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000]],\n",
       " \n",
       "         [[-160.0000,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [-160.0000,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [-160.0000,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          ...,\n",
       "          [-160.0000,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [-160.0000,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [-160.0000,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000]],\n",
       " \n",
       "         [[1435.8334,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [1435.8334,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [1435.8334,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          ...,\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1435.8334,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [1435.8334,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [1435.8334,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          ...,\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000]],\n",
       " \n",
       "         [[1435.8334,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [-160.0000,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [1435.8334,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          ...,\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000]],\n",
       " \n",
       "         [[1435.8334,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [1435.8334,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [1435.8334,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          ...,\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000]]]),\n",
       " 'pose_len': tensor([ 30, 145, 105,  36,  70,  80,  59,  36]),\n",
       " 'labels': tensor([[  1,  34,  16,   8,  23, 158,  17,  21,   4,   2,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  1,   6,  16,  64,   9,  24, 134,  41,   7,  39,  12,  31,  40,   5,\n",
       "           10,  42,  15,  27,  10, 135,  20,  35, 136,  30, 137, 138,   4,   2],\n",
       "         [  1,  58,  50,  51,  32, 220,  10,  88,  14,   7,  85,  86,   5, 221,\n",
       "           32, 222,   5, 223, 224,   5, 225,  73, 226,  14, 227, 228,   4,   2],\n",
       "         [  1,  57, 207,  15,   9,  24,  26, 208,   4,   2,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  1,  25,   8,  84,   7,  75, 213,  12,  32, 214,  43,  25,   8,  83,\n",
       "            7,  76,  33,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  1,  73,   8,  74, 154,  20,   7,  75, 155,  12,  32, 156,  17, 157,\n",
       "           12,   7,  76,  33,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  1, 209,  44,  27,  22,  17,  21,   5,   7, 210,   8,  28,  47,  10,\n",
       "           66,  14,   7, 211,   4,   2,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  1,  18, 229,  15, 230, 231,  11,  50,  51,  42,  18,   9,  48,  88,\n",
       "            4,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]]),\n",
       " 'label_len': tensor([10, 28, 28, 10, 18, 19, 20, 16]),\n",
       " 'filenames': ['data/raw_videos\\\\--8pSDeC-fg_11-5-rgb_front.mp4',\n",
       "  'data/raw_videos\\\\--7E2sU6zP4_7-5-rgb_front.mp4',\n",
       "  'data/raw_videos\\\\--dANj_01AU_1-5-rgb_front.mp4',\n",
       "  'data/raw_videos\\\\--8pSDeC-fg_5-5-rgb_front.mp4',\n",
       "  'data/raw_videos\\\\--8pSDeC-fg_9-5-rgb_front.mp4',\n",
       "  'data/raw_videos\\\\--8pSDeC-fg_10-5-rgb_front.mp4',\n",
       "  'data/raw_videos\\\\--8pSDeC-fg_6-5-rgb_front.mp4',\n",
       "  'data/raw_videos\\\\--dANj_01AU_10-5-rgb_front.mp4'],\n",
       " 'raw_labels': ['If so you have high self esteem.',\n",
       "  \"And so what's happened with the idea of acanthus leaf, is that it has is taken on all these different creative looks.\",\n",
       "  'Now make sure your client is comfortable in the side position, supporting your head, knee supported, arms are resting in front comfortably.',\n",
       "  \"But fortunately it's very fixable.\",\n",
       "  'Do you see the positive parts of your personality or do you notice the negative?',\n",
       "  'Are you always focused on the positive aspects of your physical self instead of the negative?',\n",
       "  'When one has low self esteem, the way you can tell is look in the mirror.',\n",
       "  \"I lift it up enough to make sure that I'm comfortable.\"]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Testing model",
   "id": "123f8343f7b38bcd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T21:36:06.433702Z",
     "start_time": "2025-12-02T21:36:06.417463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class PoseToTextModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pose_dim: int,          # D: feature dimension of pose per frame\n",
    "        enc_hidden: int,        # encoder GRU hidden size per direction\n",
    "        vocab_size: int,        # |V|\n",
    "        emb_dim: int,           # token embedding dim\n",
    "        pad_id: int,\n",
    "        num_enc_layers: int = 1,\n",
    "        num_dec_layers: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pad_id = pad_id\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Encoder: Bi-GRU over pose sequence\n",
    "        self.encoder = nn.GRU(\n",
    "            input_size=pose_dim,\n",
    "            hidden_size=enc_hidden,\n",
    "            num_layers=num_enc_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        # Decoder embedding\n",
    "        self.emb = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=emb_dim,\n",
    "            padding_idx=pad_id,\n",
    "        )\n",
    "\n",
    "        # Decoder GRU: hidden size = 2 * enc_hidden (concat directions)\n",
    "        self.decoder = nn.GRU(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=2 * enc_hidden,\n",
    "            num_layers=num_dec_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # Final projection to vocab\n",
    "        self.out = nn.Linear(2 * enc_hidden, vocab_size)\n",
    "\n",
    "    def encode(self, pose, pose_len):\n",
    "        \"\"\"\n",
    "        pose: [B, T, D]\n",
    "        pose_len: [B]\n",
    "        Returns: encoder final hidden state [num_layers*2, B, H]\n",
    "        \"\"\"\n",
    "        # Pack for efficient RNN\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            pose,\n",
    "            lengths=pose_len.cpu(),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "        enc_out, h_n = self.encoder(packed)\n",
    "        # h_n: [num_layers*2, B, enc_hidden]\n",
    "        return h_n\n",
    "\n",
    "    def forward(self, pose, pose_len, labels):\n",
    "        \"\"\"\n",
    "        pose:   [B, T, D]\n",
    "        pose_len: [B]\n",
    "        labels: [B, L]  (with <bos> ... <eos> and <pad>)\n",
    "\n",
    "        We use teacher forcing:\n",
    "          decoder inputs: labels[:, :-1]\n",
    "          targets:        labels[:, 1:]\n",
    "        Returns:\n",
    "          logits: [B, L-1, vocab_size]\n",
    "        \"\"\"\n",
    "        B, T, D = pose.shape\n",
    "        B2, L = labels.shape\n",
    "        assert B == B2\n",
    "\n",
    "        # ---- Encode ----\n",
    "        h_n = self.encode(pose, pose_len)  # [num_layers*2, B, enc_hidden]\n",
    "\n",
    "        # Merge directions for final layer into a single initial hidden state\n",
    "        # For simplicity, we only use last layer’s forward/backward\n",
    "        # h_n_last: [2, B, enc_hidden] -> concat -> [1, B, 2*enc_hidden]\n",
    "        num_layers_times_dir, B_enc, H = h_n.shape\n",
    "        assert B_enc == B\n",
    "        h_n_last = h_n[-2:]                 # [2, B, H] (last layer forward/backward)\n",
    "        h0_dec = torch.cat(\n",
    "            [h_n_last[0], h_n_last[1]], dim=-1\n",
    "        ).unsqueeze(0)                      # [1, B, 2H]\n",
    "\n",
    "        # ---- Decode with teacher forcing ----\n",
    "        # decoder input is labels shifted right (all but last token)\n",
    "        dec_inp = labels[:, :-1]            # [B, L-1]\n",
    "        emb = self.emb(dec_inp)             # [B, L-1, emb_dim]\n",
    "\n",
    "        dec_out, _ = self.decoder(emb, h0_dec)  # [B, L-1, 2H]\n",
    "        logits = self.out(dec_out)              # [B, L-1, vocab_size]\n",
    "\n",
    "        return logits\n"
   ],
   "id": "6f9161bc051737c0",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T21:39:22.022451Z",
     "start_time": "2025-12-02T21:39:22.013348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def build_id_to_token(vocab: dict) -> dict:\n",
    "    \"\"\"vocab: {token: id} -> {id: token}\"\"\"\n",
    "    return {idx: tok for tok, idx in vocab.items()}\n",
    "\n",
    "\n",
    "def tokens_to_text(\n",
    "    ids,\n",
    "    id_to_token,\n",
    "    pad_id: int,\n",
    "    bos_token: str = \"<bos>\",\n",
    "    eos_token: str = \"<eos>\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert a sequence of token IDs into a space-separated string.\n",
    "    Ignores <pad>, optionally removes <bos>, and stops at <eos>.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for i in ids:\n",
    "        if int(i) == pad_id:\n",
    "            continue\n",
    "        tok = id_to_token.get(int(i), \"<unk>\")\n",
    "        if tok == bos_token:\n",
    "            continue\n",
    "        if tok == eos_token:\n",
    "            break\n",
    "        tokens.append(tok)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def bleu1(pred_tokens, ref_tokens):\n",
    "    \"\"\"\n",
    "    Simple BLEU-1 (unigram BLEU) with brevity penalty.\n",
    "    pred_tokens, ref_tokens: lists of tokens (strings).\n",
    "    \"\"\"\n",
    "    if len(pred_tokens) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    pred_counts = Counter(pred_tokens)\n",
    "    ref_counts = Counter(ref_tokens)\n",
    "    overlap = sum(min(pred_counts[w], ref_counts[w]) for w in pred_counts)\n",
    "\n",
    "    precision = overlap / len(pred_tokens)\n",
    "\n",
    "    # brevity penalty\n",
    "    ref_len = len(ref_tokens)\n",
    "    pred_len = len(pred_tokens)\n",
    "    if pred_len == 0:\n",
    "        return 0.0\n",
    "    if pred_len > ref_len:\n",
    "        bp = 1.0\n",
    "    else:\n",
    "        bp = math.exp(1.0 - ref_len / pred_len)\n",
    "\n",
    "    return bp * precision\n",
    "\n",
    "\n",
    "def rouge1_f1(pred_tokens, ref_tokens):\n",
    "    \"\"\"\n",
    "    Very simple ROUGE-1 F1 (over unigrams).\n",
    "    \"\"\"\n",
    "    if not pred_tokens or not ref_tokens:\n",
    "        return 0.0\n",
    "\n",
    "    pred_counts = Counter(pred_tokens)\n",
    "    ref_counts = Counter(ref_tokens)\n",
    "\n",
    "    overlap = sum(min(pred_counts[w], ref_counts[w]) for w in pred_counts)\n",
    "\n",
    "    precision = overlap / len(pred_tokens)\n",
    "    recall = overlap / len(ref_tokens)\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return 2 * precision * recall / (precision + recall)\n"
   ],
   "id": "426a8bbaf6dac421",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T21:39:36.789028Z",
     "start_time": "2025-12-02T21:39:36.769134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pose_dim = batch[\"pose\"].shape[-1]\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "model = PoseToTextModel(\n",
    "    pose_dim=pose_dim,\n",
    "    enc_hidden=256,\n",
    "    vocab_size=vocab_size,\n",
    "    emb_dim=256,\n",
    "    pad_id=my_pad_id,\n",
    ").to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=my_pad_id)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "id_to_token = build_id_to_token(vocab)\n",
    "\n",
    "# if you used these special tokens when building vocab:\n",
    "bos_id = vocab.get(\"<bos>\", None)\n",
    "eos_id = vocab.get(\"<eos>\", None)\n"
   ],
   "id": "38366dd300ae23c9",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T21:41:40.942589Z",
     "start_time": "2025-12-02T21:39:45.956663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    total_bleu = 0.0\n",
    "    total_rouge = 0.0\n",
    "    total_sentences = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        pose = batch[\"pose\"].to(device)          # [B, T, D]\n",
    "        pose_len = batch[\"pose_len\"].to(device)  # [B]\n",
    "        labels = batch[\"labels\"].to(device)      # [B, L]\n",
    "\n",
    "        # ----- forward -----\n",
    "        logits = model(pose, pose_len, labels)   # [B, L-1, V]\n",
    "\n",
    "        # Targets are labels shifted left\n",
    "        target = labels[:, 1:]                   # [B, L-1]\n",
    "\n",
    "        B, Lm1, V = logits.shape\n",
    "        loss = loss_fn(\n",
    "            logits.reshape(B * Lm1, V),\n",
    "            target.reshape(B * Lm1),\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ----- accumulate loss (per non-pad token) -----\n",
    "        with torch.no_grad():\n",
    "            non_pad = (target != my_pad_id).sum().item()\n",
    "            non_pad = max(non_pad, 1)\n",
    "            total_loss += loss.item() * non_pad\n",
    "            total_tokens += non_pad\n",
    "\n",
    "            # ----- compute BLEU / ROUGE for this batch -----\n",
    "            # Greedy predictions: argmax over vocab\n",
    "            pred_ids_batch = logits.argmax(dim=-1)   # [B, L-1]\n",
    "            ref_ids_batch = target                   # [B, L-1]\n",
    "\n",
    "            for b in range(B):\n",
    "                pred_ids = pred_ids_batch[b].tolist()\n",
    "                ref_ids = ref_ids_batch[b].tolist()\n",
    "\n",
    "                # Convert id sequences to token sequences (strings)\n",
    "                pred_str = tokens_to_text(\n",
    "                    pred_ids,\n",
    "                    id_to_token,\n",
    "                    pad_id=my_pad_id,\n",
    "                    bos_token=\"<bos>\",\n",
    "                    eos_token=\"<eos>\",\n",
    "                )\n",
    "                ref_str = tokens_to_text(\n",
    "                    ref_ids,\n",
    "                    id_to_token,\n",
    "                    pad_id=my_pad_id,\n",
    "                    bos_token=\"<bos>\",\n",
    "                    eos_token=\"<eos>\",\n",
    "                )\n",
    "\n",
    "                pred_toks = pred_str.split()\n",
    "                ref_toks = ref_str.split()\n",
    "\n",
    "                if len(ref_toks) == 0:\n",
    "                    continue  # skip entirely empty reference\n",
    "\n",
    "                b_bleu = bleu1(pred_toks, ref_toks)\n",
    "                b_rouge = rouge1_f1(pred_toks, ref_toks)\n",
    "\n",
    "                total_bleu += b_bleu\n",
    "                total_rouge += b_rouge\n",
    "                total_sentences += 1\n",
    "\n",
    "    avg_loss = total_loss / max(total_tokens, 1)\n",
    "    avg_bleu = total_bleu / max(total_sentences, 1)\n",
    "    avg_rouge = total_rouge / max(total_sentences, 1)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch} | \"\n",
    "        f\"avg token loss: {avg_loss:.4f} | \"\n",
    "        f\"BLEU-1: {avg_bleu:.4f} | ROUGE-1 F1: {avg_rouge:.4f}\"\n",
    "    )\n"
   ],
   "id": "b67b2c8ccab75e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | avg token loss: 5.3558 | BLEU-1: 0.0561 | ROUGE-1 F1: 0.0740\n",
      "Epoch 2 | avg token loss: 4.7113 | BLEU-1: 0.1413 | ROUGE-1 F1: 0.2191\n",
      "Epoch 3 | avg token loss: 4.1272 | BLEU-1: 0.1299 | ROUGE-1 F1: 0.1901\n",
      "Epoch 4 | avg token loss: 3.6539 | BLEU-1: 0.2273 | ROUGE-1 F1: 0.2987\n",
      "Epoch 5 | avg token loss: 3.2051 | BLEU-1: 0.3029 | ROUGE-1 F1: 0.3381\n",
      "Epoch 6 | avg token loss: 2.8048 | BLEU-1: 0.3726 | ROUGE-1 F1: 0.3973\n",
      "Epoch 7 | avg token loss: 2.4183 | BLEU-1: 0.4627 | ROUGE-1 F1: 0.4767\n",
      "Epoch 8 | avg token loss: 2.0040 | BLEU-1: 0.5471 | ROUGE-1 F1: 0.5625\n",
      "Epoch 9 | avg token loss: 1.6619 | BLEU-1: 0.6141 | ROUGE-1 F1: 0.6295\n",
      "Epoch 10 | avg token loss: 1.3469 | BLEU-1: 0.7407 | ROUGE-1 F1: 0.7407\n",
      "Epoch 11 | avg token loss: 1.0729 | BLEU-1: 0.7973 | ROUGE-1 F1: 0.7973\n",
      "Epoch 12 | avg token loss: 0.8664 | BLEU-1: 0.8447 | ROUGE-1 F1: 0.8447\n",
      "Epoch 13 | avg token loss: 0.6858 | BLEU-1: 0.8784 | ROUGE-1 F1: 0.8784\n",
      "Epoch 14 | avg token loss: 0.5726 | BLEU-1: 0.8807 | ROUGE-1 F1: 0.8807\n",
      "Epoch 15 | avg token loss: 0.4827 | BLEU-1: 0.8883 | ROUGE-1 F1: 0.8883\n",
      "Epoch 16 | avg token loss: 0.4110 | BLEU-1: 0.8939 | ROUGE-1 F1: 0.8939\n",
      "Epoch 17 | avg token loss: 0.3609 | BLEU-1: 0.8992 | ROUGE-1 F1: 0.8992\n",
      "Epoch 18 | avg token loss: 0.3233 | BLEU-1: 0.9056 | ROUGE-1 F1: 0.9056\n",
      "Epoch 19 | avg token loss: 0.2922 | BLEU-1: 0.9078 | ROUGE-1 F1: 0.9078\n",
      "Epoch 20 | avg token loss: 0.2677 | BLEU-1: 0.9149 | ROUGE-1 F1: 0.9149\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6a8d3a817ebe30ee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
