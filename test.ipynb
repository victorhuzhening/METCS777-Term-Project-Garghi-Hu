{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-02T20:20:55.497450Z",
     "start_time": "2025-12-02T20:20:55.481130Z"
    }
   },
   "source": [
    "from data import *\n",
    "from pretrained_models import *\n",
    "from torch.utils.data import DataLoader\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1d1aed6d3e1aa987"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2fbf96b50de11b71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4172be24808b8cc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b29c375eebbcfb04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T20:13:04.366561Z",
     "start_time": "2025-12-02T20:13:04.300327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MediaPipeCFG = MediaPipeCfg(\"pretrained_model/hand_landmarker.task\")\n",
    "options = MediaPipeCFG.create_options()\n",
    "MP_model = MediaPipeCFG.HandLandmarker.create_from_options(options)"
   ],
   "id": "d6358d07f8fb7b45",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T20:13:04.584487Z",
     "start_time": "2025-12-02T20:13:04.383691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MMPoseCFG = MMPoseCfg(checkpoint_path='pretrained_model/checkpoint/rtmpose-s_simcc-body7_pt-body7_420e-256x192-acd4a1ef_20230504.pth',\n",
    "                      config_path='pretrained_model/mmpose_config/rtmpose_m_8xb256-420e_coco-256x192.py')\n",
    "body_model = MMPoseCFG.create_model()"
   ],
   "id": "2a4048478593bab3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: pretrained_model/checkpoint/rtmpose-s_simcc-body7_pt-body7_420e-256x192-acd4a1ef_20230504.pth\n",
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for backbone.stem.0.conv.weight: copying a param with shape torch.Size([16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([24, 3, 3, 3]).\n",
      "size mismatch for backbone.stem.0.bn.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([24]).\n",
      "size mismatch for backbone.stem.0.bn.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([24]).\n",
      "size mismatch for backbone.stem.0.bn.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([24]).\n",
      "size mismatch for backbone.stem.0.bn.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([24]).\n",
      "size mismatch for backbone.stem.1.conv.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([24, 24, 3, 3]).\n",
      "size mismatch for backbone.stem.1.bn.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([24]).\n",
      "size mismatch for backbone.stem.1.bn.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([24]).\n",
      "size mismatch for backbone.stem.1.bn.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([24]).\n",
      "size mismatch for backbone.stem.1.bn.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([24]).\n",
      "size mismatch for backbone.stem.2.conv.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([48, 24, 3, 3]).\n",
      "size mismatch for backbone.stem.2.bn.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([48]).\n",
      "size mismatch for backbone.stem.2.bn.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([48]).\n",
      "size mismatch for backbone.stem.2.bn.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([48]).\n",
      "size mismatch for backbone.stem.2.bn.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([48]).\n",
      "size mismatch for backbone.stage1.0.conv.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([96, 48, 3, 3]).\n",
      "size mismatch for backbone.stage1.0.bn.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage1.0.bn.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage1.0.bn.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage1.0.bn.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage1.1.main_conv.conv.weight: copying a param with shape torch.Size([32, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([48, 96, 1, 1]).\n",
      "size mismatch for backbone.stage1.1.main_conv.bn.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([48]).\n",
      "size mismatch for backbone.stage1.1.main_conv.bn.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([48]).\n",
      "size mismatch for backbone.stage1.1.main_conv.bn.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([48]).\n",
      "size mismatch for backbone.stage1.1.main_conv.bn.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([48]).\n",
      "size mismatch for backbone.stage1.1.short_conv.conv.weight: copying a param with shape torch.Size([32, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([48, 96, 1, 1]).\n",
      "size mismatch for backbone.stage1.1.short_conv.bn.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([48]).\n",
      "size mismatch for backbone.stage1.1.short_conv.bn.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([48]).\n",
      "size mismatch for backbone.stage1.1.short_conv.bn.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([48]).\n",
      "size mismatch for backbone.stage1.1.short_conv.bn.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([48]).\n",
      "size mismatch for backbone.stage1.1.final_conv.conv.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([96, 96, 1, 1]).\n",
      "size mismatch for backbone.stage1.1.final_conv.bn.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage1.1.final_conv.bn.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage1.1.final_conv.bn.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage1.1.final_conv.bn.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage1.1.blocks.0.conv1.conv.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([48, 48, 3, 3]).\n",
      "size mismatch for backbone.stage1.1.blocks.0.conv1.bn.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([48]).\n",
      "size mismatch for backbone.stage1.1.blocks.0.conv1.bn.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([48]).\n",
      "size mismatch for backbone.stage1.1.blocks.0.conv1.bn.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([48]).\n",
      "size mismatch for backbone.stage1.1.blocks.0.conv1.bn.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([48]).\n",
      "size mismatch for backbone.stage1.1.blocks.0.conv2.depthwise_conv.conv.weight: copying a param with shape torch.Size([32, 1, 5, 5]) from checkpoint, the shape in current model is torch.Size([48, 1, 5, 5]).\n",
      "size mismatch for backbone.stage1.1.blocks.0.conv2.depthwise_conv.bn.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([48]).\n",
      "size mismatch for backbone.stage1.1.blocks.0.conv2.depthwise_conv.bn.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([48]).\n",
      "size mismatch for backbone.stage1.1.blocks.0.conv2.depthwise_conv.bn.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([48]).\n",
      "size mismatch for backbone.stage1.1.blocks.0.conv2.depthwise_conv.bn.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([48]).\n",
      "size mismatch for backbone.stage1.1.blocks.0.conv2.pointwise_conv.conv.weight: copying a param with shape torch.Size([32, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([48, 48, 1, 1]).\n",
      "size mismatch for backbone.stage1.1.blocks.0.conv2.pointwise_conv.bn.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([48]).\n",
      "size mismatch for backbone.stage1.1.blocks.0.conv2.pointwise_conv.bn.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([48]).\n",
      "size mismatch for backbone.stage1.1.blocks.0.conv2.pointwise_conv.bn.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([48]).\n",
      "size mismatch for backbone.stage1.1.blocks.0.conv2.pointwise_conv.bn.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([48]).\n",
      "size mismatch for backbone.stage1.1.attention.fc.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([96, 96, 1, 1]).\n",
      "size mismatch for backbone.stage1.1.attention.fc.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.0.conv.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([192, 96, 3, 3]).\n",
      "size mismatch for backbone.stage2.0.bn.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage2.0.bn.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage2.0.bn.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage2.0.bn.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage2.1.main_conv.conv.weight: copying a param with shape torch.Size([64, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([96, 192, 1, 1]).\n",
      "size mismatch for backbone.stage2.1.main_conv.bn.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.main_conv.bn.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.main_conv.bn.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.main_conv.bn.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.short_conv.conv.weight: copying a param with shape torch.Size([64, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([96, 192, 1, 1]).\n",
      "size mismatch for backbone.stage2.1.short_conv.bn.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.short_conv.bn.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.short_conv.bn.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.short_conv.bn.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.final_conv.conv.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([192, 192, 1, 1]).\n",
      "size mismatch for backbone.stage2.1.final_conv.bn.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage2.1.final_conv.bn.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage2.1.final_conv.bn.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage2.1.final_conv.bn.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage2.1.blocks.0.conv1.conv.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([96, 96, 3, 3]).\n",
      "size mismatch for backbone.stage2.1.blocks.0.conv1.bn.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.blocks.0.conv1.bn.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.blocks.0.conv1.bn.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.blocks.0.conv1.bn.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.blocks.0.conv2.depthwise_conv.conv.weight: copying a param with shape torch.Size([64, 1, 5, 5]) from checkpoint, the shape in current model is torch.Size([96, 1, 5, 5]).\n",
      "size mismatch for backbone.stage2.1.blocks.0.conv2.depthwise_conv.bn.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.blocks.0.conv2.depthwise_conv.bn.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.blocks.0.conv2.depthwise_conv.bn.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.blocks.0.conv2.depthwise_conv.bn.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.blocks.0.conv2.pointwise_conv.conv.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([96, 96, 1, 1]).\n",
      "size mismatch for backbone.stage2.1.blocks.0.conv2.pointwise_conv.bn.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.blocks.0.conv2.pointwise_conv.bn.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.blocks.0.conv2.pointwise_conv.bn.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.blocks.0.conv2.pointwise_conv.bn.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.blocks.1.conv1.conv.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([96, 96, 3, 3]).\n",
      "size mismatch for backbone.stage2.1.blocks.1.conv1.bn.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.blocks.1.conv1.bn.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.blocks.1.conv1.bn.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.blocks.1.conv1.bn.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.blocks.1.conv2.depthwise_conv.conv.weight: copying a param with shape torch.Size([64, 1, 5, 5]) from checkpoint, the shape in current model is torch.Size([96, 1, 5, 5]).\n",
      "size mismatch for backbone.stage2.1.blocks.1.conv2.depthwise_conv.bn.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.blocks.1.conv2.depthwise_conv.bn.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.blocks.1.conv2.depthwise_conv.bn.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.blocks.1.conv2.depthwise_conv.bn.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.blocks.1.conv2.pointwise_conv.conv.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([96, 96, 1, 1]).\n",
      "size mismatch for backbone.stage2.1.blocks.1.conv2.pointwise_conv.bn.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.blocks.1.conv2.pointwise_conv.bn.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.blocks.1.conv2.pointwise_conv.bn.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.blocks.1.conv2.pointwise_conv.bn.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([96]).\n",
      "size mismatch for backbone.stage2.1.attention.fc.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([192, 192, 1, 1]).\n",
      "size mismatch for backbone.stage2.1.attention.fc.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.0.conv.weight: copying a param with shape torch.Size([256, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([384, 192, 3, 3]).\n",
      "size mismatch for backbone.stage3.0.bn.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage3.0.bn.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage3.0.bn.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage3.0.bn.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage3.1.main_conv.conv.weight: copying a param with shape torch.Size([128, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([192, 384, 1, 1]).\n",
      "size mismatch for backbone.stage3.1.main_conv.bn.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.main_conv.bn.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.main_conv.bn.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.main_conv.bn.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.short_conv.conv.weight: copying a param with shape torch.Size([128, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([192, 384, 1, 1]).\n",
      "size mismatch for backbone.stage3.1.short_conv.bn.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.short_conv.bn.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.short_conv.bn.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.short_conv.bn.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.final_conv.conv.weight: copying a param with shape torch.Size([256, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([384, 384, 1, 1]).\n",
      "size mismatch for backbone.stage3.1.final_conv.bn.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage3.1.final_conv.bn.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage3.1.final_conv.bn.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage3.1.final_conv.bn.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage3.1.blocks.0.conv1.conv.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([192, 192, 3, 3]).\n",
      "size mismatch for backbone.stage3.1.blocks.0.conv1.bn.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.blocks.0.conv1.bn.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.blocks.0.conv1.bn.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.blocks.0.conv1.bn.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.blocks.0.conv2.depthwise_conv.conv.weight: copying a param with shape torch.Size([128, 1, 5, 5]) from checkpoint, the shape in current model is torch.Size([192, 1, 5, 5]).\n",
      "size mismatch for backbone.stage3.1.blocks.0.conv2.depthwise_conv.bn.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.blocks.0.conv2.depthwise_conv.bn.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.blocks.0.conv2.depthwise_conv.bn.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.blocks.0.conv2.depthwise_conv.bn.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.blocks.0.conv2.pointwise_conv.conv.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([192, 192, 1, 1]).\n",
      "size mismatch for backbone.stage3.1.blocks.0.conv2.pointwise_conv.bn.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.blocks.0.conv2.pointwise_conv.bn.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.blocks.0.conv2.pointwise_conv.bn.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.blocks.0.conv2.pointwise_conv.bn.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.blocks.1.conv1.conv.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([192, 192, 3, 3]).\n",
      "size mismatch for backbone.stage3.1.blocks.1.conv1.bn.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.blocks.1.conv1.bn.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.blocks.1.conv1.bn.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.blocks.1.conv1.bn.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.blocks.1.conv2.depthwise_conv.conv.weight: copying a param with shape torch.Size([128, 1, 5, 5]) from checkpoint, the shape in current model is torch.Size([192, 1, 5, 5]).\n",
      "size mismatch for backbone.stage3.1.blocks.1.conv2.depthwise_conv.bn.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.blocks.1.conv2.depthwise_conv.bn.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.blocks.1.conv2.depthwise_conv.bn.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.blocks.1.conv2.depthwise_conv.bn.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.blocks.1.conv2.pointwise_conv.conv.weight: copying a param with shape torch.Size([128, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([192, 192, 1, 1]).\n",
      "size mismatch for backbone.stage3.1.blocks.1.conv2.pointwise_conv.bn.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.blocks.1.conv2.pointwise_conv.bn.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.blocks.1.conv2.pointwise_conv.bn.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.blocks.1.conv2.pointwise_conv.bn.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([192]).\n",
      "size mismatch for backbone.stage3.1.attention.fc.weight: copying a param with shape torch.Size([256, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([384, 384, 1, 1]).\n",
      "size mismatch for backbone.stage3.1.attention.fc.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage4.0.conv.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([768, 384, 3, 3]).\n",
      "size mismatch for backbone.stage4.0.bn.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "size mismatch for backbone.stage4.0.bn.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "size mismatch for backbone.stage4.0.bn.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "size mismatch for backbone.stage4.0.bn.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "size mismatch for backbone.stage4.1.conv1.conv.weight: copying a param with shape torch.Size([256, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([384, 768, 1, 1]).\n",
      "size mismatch for backbone.stage4.1.conv1.bn.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage4.1.conv1.bn.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage4.1.conv1.bn.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage4.1.conv1.bn.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage4.1.conv2.conv.weight: copying a param with shape torch.Size([512, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([768, 1536, 1, 1]).\n",
      "size mismatch for backbone.stage4.1.conv2.bn.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "size mismatch for backbone.stage4.1.conv2.bn.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "size mismatch for backbone.stage4.1.conv2.bn.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "size mismatch for backbone.stage4.1.conv2.bn.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "size mismatch for backbone.stage4.2.main_conv.conv.weight: copying a param with shape torch.Size([256, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([384, 768, 1, 1]).\n",
      "size mismatch for backbone.stage4.2.main_conv.bn.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage4.2.main_conv.bn.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage4.2.main_conv.bn.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage4.2.main_conv.bn.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage4.2.short_conv.conv.weight: copying a param with shape torch.Size([256, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([384, 768, 1, 1]).\n",
      "size mismatch for backbone.stage4.2.short_conv.bn.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage4.2.short_conv.bn.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage4.2.short_conv.bn.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage4.2.short_conv.bn.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage4.2.final_conv.conv.weight: copying a param with shape torch.Size([512, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([768, 768, 1, 1]).\n",
      "size mismatch for backbone.stage4.2.final_conv.bn.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "size mismatch for backbone.stage4.2.final_conv.bn.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "size mismatch for backbone.stage4.2.final_conv.bn.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "size mismatch for backbone.stage4.2.final_conv.bn.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "size mismatch for backbone.stage4.2.blocks.0.conv1.conv.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([384, 384, 3, 3]).\n",
      "size mismatch for backbone.stage4.2.blocks.0.conv1.bn.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage4.2.blocks.0.conv1.bn.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage4.2.blocks.0.conv1.bn.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage4.2.blocks.0.conv1.bn.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage4.2.blocks.0.conv2.depthwise_conv.conv.weight: copying a param with shape torch.Size([256, 1, 5, 5]) from checkpoint, the shape in current model is torch.Size([384, 1, 5, 5]).\n",
      "size mismatch for backbone.stage4.2.blocks.0.conv2.depthwise_conv.bn.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage4.2.blocks.0.conv2.depthwise_conv.bn.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage4.2.blocks.0.conv2.depthwise_conv.bn.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage4.2.blocks.0.conv2.depthwise_conv.bn.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage4.2.blocks.0.conv2.pointwise_conv.conv.weight: copying a param with shape torch.Size([256, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([384, 384, 1, 1]).\n",
      "size mismatch for backbone.stage4.2.blocks.0.conv2.pointwise_conv.bn.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage4.2.blocks.0.conv2.pointwise_conv.bn.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage4.2.blocks.0.conv2.pointwise_conv.bn.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage4.2.blocks.0.conv2.pointwise_conv.bn.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([384]).\n",
      "size mismatch for backbone.stage4.2.attention.fc.weight: copying a param with shape torch.Size([512, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([768, 768, 1, 1]).\n",
      "size mismatch for backbone.stage4.2.attention.fc.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n",
      "size mismatch for head.final_layer.weight: copying a param with shape torch.Size([17, 512, 7, 7]) from checkpoint, the shape in current model is torch.Size([17, 768, 7, 7]).\n",
      "missing keys in source state_dict: backbone.stage1.1.blocks.1.conv1.conv.weight, backbone.stage1.1.blocks.1.conv1.bn.weight, backbone.stage1.1.blocks.1.conv1.bn.bias, backbone.stage1.1.blocks.1.conv1.bn.running_mean, backbone.stage1.1.blocks.1.conv1.bn.running_var, backbone.stage1.1.blocks.1.conv2.depthwise_conv.conv.weight, backbone.stage1.1.blocks.1.conv2.depthwise_conv.bn.weight, backbone.stage1.1.blocks.1.conv2.depthwise_conv.bn.bias, backbone.stage1.1.blocks.1.conv2.depthwise_conv.bn.running_mean, backbone.stage1.1.blocks.1.conv2.depthwise_conv.bn.running_var, backbone.stage1.1.blocks.1.conv2.pointwise_conv.conv.weight, backbone.stage1.1.blocks.1.conv2.pointwise_conv.bn.weight, backbone.stage1.1.blocks.1.conv2.pointwise_conv.bn.bias, backbone.stage1.1.blocks.1.conv2.pointwise_conv.bn.running_mean, backbone.stage1.1.blocks.1.conv2.pointwise_conv.bn.running_var, backbone.stage2.1.blocks.2.conv1.conv.weight, backbone.stage2.1.blocks.2.conv1.bn.weight, backbone.stage2.1.blocks.2.conv1.bn.bias, backbone.stage2.1.blocks.2.conv1.bn.running_mean, backbone.stage2.1.blocks.2.conv1.bn.running_var, backbone.stage2.1.blocks.2.conv2.depthwise_conv.conv.weight, backbone.stage2.1.blocks.2.conv2.depthwise_conv.bn.weight, backbone.stage2.1.blocks.2.conv2.depthwise_conv.bn.bias, backbone.stage2.1.blocks.2.conv2.depthwise_conv.bn.running_mean, backbone.stage2.1.blocks.2.conv2.depthwise_conv.bn.running_var, backbone.stage2.1.blocks.2.conv2.pointwise_conv.conv.weight, backbone.stage2.1.blocks.2.conv2.pointwise_conv.bn.weight, backbone.stage2.1.blocks.2.conv2.pointwise_conv.bn.bias, backbone.stage2.1.blocks.2.conv2.pointwise_conv.bn.running_mean, backbone.stage2.1.blocks.2.conv2.pointwise_conv.bn.running_var, backbone.stage2.1.blocks.3.conv1.conv.weight, backbone.stage2.1.blocks.3.conv1.bn.weight, backbone.stage2.1.blocks.3.conv1.bn.bias, backbone.stage2.1.blocks.3.conv1.bn.running_mean, backbone.stage2.1.blocks.3.conv1.bn.running_var, backbone.stage2.1.blocks.3.conv2.depthwise_conv.conv.weight, backbone.stage2.1.blocks.3.conv2.depthwise_conv.bn.weight, backbone.stage2.1.blocks.3.conv2.depthwise_conv.bn.bias, backbone.stage2.1.blocks.3.conv2.depthwise_conv.bn.running_mean, backbone.stage2.1.blocks.3.conv2.depthwise_conv.bn.running_var, backbone.stage2.1.blocks.3.conv2.pointwise_conv.conv.weight, backbone.stage2.1.blocks.3.conv2.pointwise_conv.bn.weight, backbone.stage2.1.blocks.3.conv2.pointwise_conv.bn.bias, backbone.stage2.1.blocks.3.conv2.pointwise_conv.bn.running_mean, backbone.stage2.1.blocks.3.conv2.pointwise_conv.bn.running_var, backbone.stage3.1.blocks.2.conv1.conv.weight, backbone.stage3.1.blocks.2.conv1.bn.weight, backbone.stage3.1.blocks.2.conv1.bn.bias, backbone.stage3.1.blocks.2.conv1.bn.running_mean, backbone.stage3.1.blocks.2.conv1.bn.running_var, backbone.stage3.1.blocks.2.conv2.depthwise_conv.conv.weight, backbone.stage3.1.blocks.2.conv2.depthwise_conv.bn.weight, backbone.stage3.1.blocks.2.conv2.depthwise_conv.bn.bias, backbone.stage3.1.blocks.2.conv2.depthwise_conv.bn.running_mean, backbone.stage3.1.blocks.2.conv2.depthwise_conv.bn.running_var, backbone.stage3.1.blocks.2.conv2.pointwise_conv.conv.weight, backbone.stage3.1.blocks.2.conv2.pointwise_conv.bn.weight, backbone.stage3.1.blocks.2.conv2.pointwise_conv.bn.bias, backbone.stage3.1.blocks.2.conv2.pointwise_conv.bn.running_mean, backbone.stage3.1.blocks.2.conv2.pointwise_conv.bn.running_var, backbone.stage3.1.blocks.3.conv1.conv.weight, backbone.stage3.1.blocks.3.conv1.bn.weight, backbone.stage3.1.blocks.3.conv1.bn.bias, backbone.stage3.1.blocks.3.conv1.bn.running_mean, backbone.stage3.1.blocks.3.conv1.bn.running_var, backbone.stage3.1.blocks.3.conv2.depthwise_conv.conv.weight, backbone.stage3.1.blocks.3.conv2.depthwise_conv.bn.weight, backbone.stage3.1.blocks.3.conv2.depthwise_conv.bn.bias, backbone.stage3.1.blocks.3.conv2.depthwise_conv.bn.running_mean, backbone.stage3.1.blocks.3.conv2.depthwise_conv.bn.running_var, backbone.stage3.1.blocks.3.conv2.pointwise_conv.conv.weight, backbone.stage3.1.blocks.3.conv2.pointwise_conv.bn.weight, backbone.stage3.1.blocks.3.conv2.pointwise_conv.bn.bias, backbone.stage3.1.blocks.3.conv2.pointwise_conv.bn.running_mean, backbone.stage3.1.blocks.3.conv2.pointwise_conv.bn.running_var, backbone.stage4.2.blocks.1.conv1.conv.weight, backbone.stage4.2.blocks.1.conv1.bn.weight, backbone.stage4.2.blocks.1.conv1.bn.bias, backbone.stage4.2.blocks.1.conv1.bn.running_mean, backbone.stage4.2.blocks.1.conv1.bn.running_var, backbone.stage4.2.blocks.1.conv2.depthwise_conv.conv.weight, backbone.stage4.2.blocks.1.conv2.depthwise_conv.bn.weight, backbone.stage4.2.blocks.1.conv2.depthwise_conv.bn.bias, backbone.stage4.2.blocks.1.conv2.depthwise_conv.bn.running_mean, backbone.stage4.2.blocks.1.conv2.depthwise_conv.bn.running_var, backbone.stage4.2.blocks.1.conv2.pointwise_conv.conv.weight, backbone.stage4.2.blocks.1.conv2.pointwise_conv.bn.weight, backbone.stage4.2.blocks.1.conv2.pointwise_conv.bn.bias, backbone.stage4.2.blocks.1.conv2.pointwise_conv.bn.running_mean, backbone.stage4.2.blocks.1.conv2.pointwise_conv.bn.running_var\n",
      "\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T20:13:04.933201Z",
     "start_time": "2025-12-02T20:13:04.616577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PreTrainDataset = How2Sign(video_dir=\"data/raw_videos\",\n",
    "                           MP_model=MP_model,\n",
    "                           body_cfg=MMPoseCFG,\n",
    "                           body_model=body_model,\n",
    "                           labels_path=\"data/how2sign_realigned_train.csv\")\n",
    "def collate_as_is(batch):\n",
    "    return batch\n",
    "PreTrainDataLoader = DataLoader(PreTrainDataset, batch_size=4, shuffle=False, collate_fn=collate_as_is)"
   ],
   "id": "17db5e3ecba609bd",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T20:15:50.202330Z",
     "start_time": "2025-12-02T20:13:04.950278Z"
    }
   },
   "cell_type": "code",
   "source": "OneSample = next(iter(PreTrainDataLoader))",
   "id": "29fbf0c787ba7902",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "One batch runs in 2 m 45 secs",
   "id": "93bc1d6c0fced99c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Test Conversion To JSON Code",
   "id": "a941e9ff5fa96aa0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T20:23:43.183264Z",
     "start_time": "2025-12-02T20:23:43.175339Z"
    }
   },
   "cell_type": "code",
   "source": "type(OneSample[0]['hand_landmarks'])",
   "id": "3fed63731b862236",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Write converted JSON to local dir for testing",
   "id": "99e231e4bc1e3690"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T20:21:01.173853Z",
     "start_time": "2025-12-02T20:21:01.157364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_dir = \"test_json_8_batches\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ],
   "id": "8826148f83d87ef2",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T20:38:12.306740Z",
     "start_time": "2025-12-02T20:24:19.501820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_json_safe(x):\n",
    "    \"\"\"\n",
    "    Recursively convert tensors, numpy arrays, and other non-serializable\n",
    "    objects into plain Python lists or scalars.\n",
    "    \"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.cpu().numpy().tolist()\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.tolist()\n",
    "    if isinstance(x, dict):\n",
    "        return {k: make_json_safe(v) for k, v in x.items()}\n",
    "    if isinstance(x, list):\n",
    "        return [make_json_safe(v) for v in x]\n",
    "    if isinstance(x, tuple):\n",
    "        return tuple(make_json_safe(v) for v in x)\n",
    "    return x  # scalar, string, or already json-safe\n",
    "\n",
    "print(\"Saving dataloader batches to:\", output_dir)\n",
    "\n",
    "for batch_idx, batch in enumerate(PreTrainDataLoader):\n",
    "    if batch_idx == 8:\n",
    "        break\n",
    "    json_safe_batch = make_json_safe(batch)\n",
    "\n",
    "    output_path = os.path.join(output_dir, f\"batch_{batch_idx:05d}.json\")\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_safe_batch, f, indent=2)\n",
    "\n",
    "    print(f\"[✓] Saved batch {batch_idx} → {output_path}\")\n",
    "\n",
    "\n",
    "print(\"Finished writing all batches!\")"
   ],
   "id": "a9f3796bddc43f72",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataloader batches to: test_json_8_batches\n",
      "[✓] Saved batch 0 → test_json_8_batches\\batch_00000.json\n",
      "[✓] Saved batch 1 → test_json_8_batches\\batch_00001.json\n",
      "[✓] Saved batch 2 → test_json_8_batches\\batch_00002.json\n",
      "[✓] Saved batch 3 → test_json_8_batches\\batch_00003.json\n",
      "[✓] Saved batch 4 → test_json_8_batches\\batch_00004.json\n",
      "[✓] Saved batch 5 → test_json_8_batches\\batch_00005.json\n",
      "[✓] Saved batch 6 → test_json_8_batches\\batch_00006.json\n",
      "[✓] Saved batch 7 → test_json_8_batches\\batch_00007.json\n",
      "Finished writing all batches!\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "8 batches runs in 13m 52s",
   "id": "1afb2375729c8367"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Read in json data into trainable data using Custom DataLoader and Text Tokenizer",
   "id": "96ad6111943884ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T21:32:04.217650Z",
     "start_time": "2025-12-02T21:32:04.193120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "from typing import Callable, List, Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class ASLPoseJSONDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for ASL pose + hand landmarks + text label stored in JSON.\n",
    "\n",
    "    Each JSON file in `json_dir` is either:\n",
    "      - a single sample dict, or\n",
    "      - a list of sample dicts (e.g., saved from a DataLoader batch)\n",
    "\n",
    "    Expected per-sample structure:\n",
    "      {\n",
    "        \"filename\": str,\n",
    "        \"hand_landmarks\": {\n",
    "            \"num_frames\": int,\n",
    "            \"frames\": [\n",
    "                {\n",
    "                    \"frame_index\": int,\n",
    "                    \"hands\": [\n",
    "                        {\n",
    "                            \"handedness\": \"Left\" | \"Right\",\n",
    "                            \"landmarks\": [\n",
    "                                {\"x\": float, \"y\": float, \"z\": float},  # 21 of these\n",
    "                                ...\n",
    "                            ]\n",
    "                        },\n",
    "                        ...\n",
    "                    ]\n",
    "                },\n",
    "                ...\n",
    "            ]\n",
    "        },\n",
    "        \"body_coordinates\": {\n",
    "            \"num_frames\": int,\n",
    "            \"frames\": [\n",
    "                {\n",
    "                    \"frame_index\": int,\n",
    "                    \"instances\": [\n",
    "                        {\n",
    "                            \"pred_instances\": {\n",
    "                                \"keypoints\": ...,\n",
    "                                \"keypoint_scores\": ...,\n",
    "                                # \"scores\", \"bboxes\" may also exist\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                ...\n",
    "            ]\n",
    "        },\n",
    "        \"label\": \"english sentence\"\n",
    "      }\n",
    "\n",
    "    Args:\n",
    "        json_dir: directory containing *.json files.\n",
    "        tokenizer: callable that maps text -> List[int]\n",
    "        max_frames: optional max number of frames per sample (after subsample).\n",
    "        frame_subsample: take every k-th frame (temporal downsampling).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        json_dir: str,\n",
    "        tokenizer: Callable[[str], List[int]],\n",
    "        max_frames: int = None,\n",
    "        frame_subsample: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.json_dir = json_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_frames = max_frames\n",
    "        self.frame_subsample = max(1, int(frame_subsample))\n",
    "\n",
    "        self.json_paths = sorted(glob(os.path.join(json_dir, \"*.json\")))\n",
    "        if not self.json_paths:\n",
    "            raise RuntimeError(f\"No JSON files found in directory: {json_dir}\")\n",
    "\n",
    "        # index is a list of (path, inner_idx)\n",
    "        # inner_idx is None if the file contains a single dict\n",
    "        self.index: List[Tuple[str, int | None]] = []\n",
    "        for path in self.json_paths:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            if isinstance(data, list):\n",
    "                for i in range(len(data)):\n",
    "                    self.index.append((path, i))\n",
    "            elif isinstance(data, dict):\n",
    "                self.index.append((path, None))\n",
    "            else:\n",
    "                raise TypeError(\n",
    "                    f\"Unexpected JSON top-level type in {path}: {type(data)}\"\n",
    "                )\n",
    "\n",
    "        if len(self.index) == 0:\n",
    "            raise RuntimeError(f\"JSON dir {json_dir} contained no valid samples.\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.index)\n",
    "\n",
    "    def _load_sample(self, path: str, inner_idx: int | None) -> Dict[str, Any]:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        if isinstance(data, list):\n",
    "            if inner_idx is None:\n",
    "                raise RuntimeError(\"inner_idx is None for a list-based JSON file.\")\n",
    "            sample = data[inner_idx]\n",
    "        elif isinstance(data, dict):\n",
    "            sample = data\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                f\"Unexpected JSON top-level type in {path}: {type(data)}\"\n",
    "            )\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        path, inner_idx = self.index[idx]\n",
    "        data = self._load_sample(path, inner_idx)\n",
    "\n",
    "        filename = data.get(\"filename\", \"\")\n",
    "        label_text = data.get(\"label\", \"\")\n",
    "\n",
    "        if \"hand_landmarks\" not in data or \"body_coordinates\" not in data:\n",
    "            raise KeyError(\n",
    "                f\"Sample in {path} missing 'hand_landmarks' or 'body_coordinates'\"\n",
    "            )\n",
    "\n",
    "        hand = data[\"hand_landmarks\"]\n",
    "        body = data[\"body_coordinates\"]\n",
    "\n",
    "        # --- determine frame count & indices ---\n",
    "        T_hand = hand.get(\"num_frames\", len(hand.get(\"frames\", [])))\n",
    "        T_body = body.get(\"num_frames\", len(body.get(\"frames\", [])))\n",
    "        T = min(T_hand, T_body)\n",
    "\n",
    "        if T == 0:\n",
    "            raise RuntimeError(f\"Sample in {path} has zero frames.\")\n",
    "\n",
    "        frames_hand = hand[\"frames\"]\n",
    "        frames_body = body[\"frames\"]\n",
    "\n",
    "        frame_indices = list(range(0, T, self.frame_subsample))\n",
    "        if self.max_frames is not None:\n",
    "            frame_indices = frame_indices[: self.max_frames]\n",
    "\n",
    "        feature_seq: List[np.ndarray] = []\n",
    "\n",
    "        for t in frame_indices:\n",
    "            # --- body features ---\n",
    "            body_frame = frames_body[t]\n",
    "            instances = body_frame.get(\"instances\", [])\n",
    "            if not instances:\n",
    "                # If no body detected, you could choose to skip this frame.\n",
    "                # For now, we raise to surface the data issue.\n",
    "                raise RuntimeError(\n",
    "                    f\"No 'instances' in body frame t={t} in sample from {path}\"\n",
    "                )\n",
    "\n",
    "            body_pi = instances[0].get(\"pred_instances\", {})\n",
    "\n",
    "            body_kpts = np.array(body_pi.get(\"keypoints\", []), dtype=np.float32)\n",
    "            body_scores = np.array(body_pi.get(\"keypoint_scores\", []), dtype=np.float32)\n",
    "\n",
    "            # Flatten to 1D\n",
    "            body_xy_flat = body_kpts.reshape(-1)\n",
    "            body_scores_flat = body_scores.reshape(-1)\n",
    "\n",
    "            # --- hand features ---\n",
    "            frame_entry = frames_hand[t]\n",
    "            hands_list = frame_entry.get(\"hands\", [])\n",
    "            if not isinstance(hands_list, list):\n",
    "                hands_list = []\n",
    "\n",
    "            # Default: both hands zeros [21,3]\n",
    "            left_hand = np.zeros((21, 3), dtype=np.float32)\n",
    "            right_hand = np.zeros((21, 3), dtype=np.float32)\n",
    "\n",
    "            for h in hands_list:\n",
    "                lm_raw = h.get(\"landmarks\", [])\n",
    "                if not lm_raw:\n",
    "                    continue\n",
    "\n",
    "                # landmarks could be dicts or already [x,y,z] lists\n",
    "                first = lm_raw[0]\n",
    "                if isinstance(first, dict):\n",
    "                    lm = np.array(\n",
    "                        [[p[\"x\"], p[\"y\"], p[\"z\"]] for p in lm_raw],\n",
    "                        dtype=np.float32,\n",
    "                    )\n",
    "                else:\n",
    "                    lm = np.array(lm_raw, dtype=np.float32)\n",
    "\n",
    "                if lm.shape != (21, 3):\n",
    "                    # If MediaPipe changes, you may want to relax this.\n",
    "                    lm = lm.reshape(21, 3)\n",
    "\n",
    "                handed = h.get(\"handedness\", \"Unknown\")\n",
    "                if handed == \"Left\":\n",
    "                    left_hand = lm\n",
    "                elif handed == \"Right\":\n",
    "                    right_hand = lm\n",
    "\n",
    "            left_flat = left_hand.reshape(-1)   # [63]\n",
    "            right_flat = right_hand.reshape(-1) # [63]\n",
    "\n",
    "            # Concatenate everything into one per-frame feature vector\n",
    "            feature_vec = np.concatenate(\n",
    "                [body_xy_flat, body_scores_flat, left_flat, right_flat],\n",
    "                axis=0,\n",
    "            )\n",
    "            feature_seq.append(feature_vec)\n",
    "\n",
    "        if not feature_seq:\n",
    "            raise RuntimeError(f\"No usable frames for sample in {path}.\")\n",
    "\n",
    "        feature_seq_np = np.stack(feature_seq, axis=0)  # [T', D]\n",
    "        pose = torch.from_numpy(feature_seq_np).float()\n",
    "        pose_len = pose.shape[0]\n",
    "\n",
    "        # --- tokenize label ---\n",
    "        label_ids = self.tokenizer(label_text)  # List[int]\n",
    "        label_ids = torch.tensor(label_ids, dtype=torch.long)\n",
    "        label_len = label_ids.shape[0]\n",
    "\n",
    "        return {\n",
    "            \"pose\": pose,                    # [T', D]\n",
    "            \"pose_len\": pose_len,            # scalar\n",
    "            \"label_ids\": label_ids,          # [L]\n",
    "            \"label_len\": label_len,          # scalar\n",
    "            \"filename\": filename,\n",
    "            \"raw_label\": label_text,\n",
    "        }\n",
    "\n",
    "\n",
    "def asl_collate_fn(batch: List[Dict[str, Any]], pad_id: int) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Collate function for ASLPoseJSONDataset.\n",
    "\n",
    "    Pads pose sequences in time dimension, and label sequences in length,\n",
    "    using pad_id for text.\n",
    "    \"\"\"\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    pose_lens = [b[\"pose_len\"] for b in batch]\n",
    "    label_lens = [b[\"label_len\"] for b in batch]\n",
    "\n",
    "    max_T = max(pose_lens)\n",
    "    max_L = max(label_lens)\n",
    "\n",
    "    D = batch[0][\"pose\"].shape[1]  # feature dim\n",
    "\n",
    "    pose_batch = torch.zeros(batch_size, max_T, D, dtype=torch.float32)\n",
    "    label_batch = torch.full(\n",
    "        (batch_size, max_L), fill_value=pad_id, dtype=torch.long\n",
    "    )\n",
    "\n",
    "    pose_len_tensor = torch.tensor(pose_lens, dtype=torch.long)\n",
    "    label_len_tensor = torch.tensor(label_lens, dtype=torch.long)\n",
    "\n",
    "    filenames = []\n",
    "    raw_labels = []\n",
    "\n",
    "    for i, sample in enumerate(batch):\n",
    "        T = sample[\"pose_len\"]\n",
    "        L = sample[\"label_len\"]\n",
    "\n",
    "        pose_batch[i, :T] = sample[\"pose\"]\n",
    "        label_batch[i, :L] = sample[\"label_ids\"]\n",
    "\n",
    "        filenames.append(sample[\"filename\"])\n",
    "        raw_labels.append(sample[\"raw_label\"])\n",
    "\n",
    "    return {\n",
    "        \"pose\": pose_batch,          # [B, max_T, D]\n",
    "        \"pose_len\": pose_len_tensor, # [B]\n",
    "        \"labels\": label_batch,       # [B, max_L]\n",
    "        \"label_len\": label_len_tensor,  # [B]\n",
    "        \"filenames\": filenames,\n",
    "        \"raw_labels\": raw_labels,\n",
    "    }\n"
   ],
   "id": "dcc16b8fecb5ce56",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T21:32:08.018351Z",
     "start_time": "2025-12-02T21:32:07.996069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tokenizer_simple.py\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "\n",
    "TOKEN_PATTERN = re.compile(r\"\\w+|[^\\w\\s]\", re.UNICODE)\n",
    "\n",
    "def basic_tokenize(text: str):\n",
    "    text = text.lower()\n",
    "    return TOKEN_PATTERN.findall(text)\n",
    "\n",
    "\n",
    "def label_iterator(json_dir: str):\n",
    "    \"\"\"\n",
    "    Iterate over tokenized labels in all JSON files.\n",
    "    Supports:\n",
    "      - file contains a single sample dict\n",
    "      - file contains a list of sample dicts (batches)\n",
    "    \"\"\"\n",
    "    json_paths = sorted(glob(os.path.join(json_dir, \"*.json\")))\n",
    "    if not json_paths:\n",
    "        print(f\"[label_iterator] WARNING: no *.json files found in {json_dir}\")\n",
    "\n",
    "    for path in json_paths:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Case 1: one sample per file\n",
    "        if isinstance(data, dict):\n",
    "            label = data[\"label\"]\n",
    "            yield basic_tokenize(label)\n",
    "\n",
    "        # Case 2: batch file: list of samples\n",
    "        elif isinstance(data, list):\n",
    "            for sample in data:\n",
    "                if not isinstance(sample, dict):\n",
    "                    continue\n",
    "                if \"label\" not in sample:\n",
    "                    continue\n",
    "                label = sample[\"label\"]\n",
    "                yield basic_tokenize(label)\n",
    "\n",
    "        else:\n",
    "            # Unexpected structure – skip\n",
    "            continue\n",
    "\n",
    "\n",
    "def build_asl_vocab(json_dir: str, min_freq: int = 1):\n",
    "    counter = Counter()\n",
    "    for tokens in label_iterator(json_dir):\n",
    "        counter.update(tokens)\n",
    "\n",
    "    vocab = {\n",
    "        \"<pad>\": 0,\n",
    "        \"<bos>\": 1,\n",
    "        \"<eos>\": 2,\n",
    "        \"<unk>\": 3,\n",
    "    }\n",
    "\n",
    "    for token, freq in counter.most_common():\n",
    "        if freq < min_freq:\n",
    "            continue\n",
    "        if token not in vocab:\n",
    "            vocab[token] = len(vocab)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def create_tokenizer(json_dir: str, min_freq: int = 1):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      tokenizer_fn(text) -> List[int]\n",
    "      vocab: dict {token: id}\n",
    "      pad_id: int\n",
    "    \"\"\"\n",
    "    vocab = build_asl_vocab(json_dir, min_freq=min_freq)\n",
    "    unk_id = vocab[\"<unk>\"]\n",
    "\n",
    "    def tokenizer_fn(text: str):\n",
    "        tokens = [\"<bos>\"] + basic_tokenize(text) + [\"<eos>\"]\n",
    "        return [vocab.get(tok, unk_id) for tok in tokens]\n",
    "\n",
    "    pad_id = vocab[\"<pad>\"]\n",
    "    return tokenizer_fn, vocab, pad_id"
   ],
   "id": "9b2c0ed57239fee0",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T21:32:11.284772Z",
     "start_time": "2025-12-02T21:32:08.901181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Build tokenizer & vocab from your saved JSONs\n",
    "tokenizer_fn, vocab, my_pad_id = create_tokenizer(\n",
    "    json_dir=\"test_json_8_batches\",  # or wherever you dumped them\n",
    "    min_freq=1                       # or >1 to prune rare tokens\n",
    ")\n",
    "\n",
    "print(\"PAD ID:\", my_pad_id)\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "\n",
    "# Create dataset\n",
    "dataset = ASLPoseJSONDataset(\n",
    "    json_dir=\"test_json_8_batches\",\n",
    "    tokenizer=tokenizer_fn,\n",
    "    max_frames=300,\n",
    "    frame_subsample=2,\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda b: asl_collate_fn(b, pad_id=my_pad_id),\n",
    ")\n"
   ],
   "id": "29e83141dfdd773a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAD ID: 0\n",
      "Vocab size: 239\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T21:32:12.386211Z",
     "start_time": "2025-12-02T21:32:11.302004Z"
    }
   },
   "cell_type": "code",
   "source": "next(iter(loader))",
   "id": "1d3f87862d4ee6fb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pose': tensor([[[1435.8334,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [1435.8334,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [1435.8334,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          ...,\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000]],\n",
       " \n",
       "         [[-160.0000,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [-160.0000,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [-160.0000,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          ...,\n",
       "          [-160.0000,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [-160.0000,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [-160.0000,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000]],\n",
       " \n",
       "         [[1435.8334,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [1435.8334,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [1435.8334,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          ...,\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[1435.8334,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [1435.8334,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [1435.8334,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          ...,\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000]],\n",
       " \n",
       "         [[1435.8334,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [-160.0000,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [1435.8334,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          ...,\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000]],\n",
       " \n",
       "         [[1435.8334,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [1435.8334,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [1435.8334,  310.0000,  398.3333,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          ...,\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000],\n",
       "          [   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
       "              0.0000]]]),\n",
       " 'pose_len': tensor([ 30, 145, 105,  36,  70,  80,  59,  36]),\n",
       " 'labels': tensor([[  1,  34,  16,   8,  23, 158,  17,  21,   4,   2,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  1,   6,  16,  64,   9,  24, 134,  41,   7,  39,  12,  31,  40,   5,\n",
       "           10,  42,  15,  27,  10, 135,  20,  35, 136,  30, 137, 138,   4,   2],\n",
       "         [  1,  58,  50,  51,  32, 220,  10,  88,  14,   7,  85,  86,   5, 221,\n",
       "           32, 222,   5, 223, 224,   5, 225,  73, 226,  14, 227, 228,   4,   2],\n",
       "         [  1,  57, 207,  15,   9,  24,  26, 208,   4,   2,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  1,  25,   8,  84,   7,  75, 213,  12,  32, 214,  43,  25,   8,  83,\n",
       "            7,  76,  33,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  1,  73,   8,  74, 154,  20,   7,  75, 155,  12,  32, 156,  17, 157,\n",
       "           12,   7,  76,  33,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  1, 209,  44,  27,  22,  17,  21,   5,   7, 210,   8,  28,  47,  10,\n",
       "           66,  14,   7, 211,   4,   2,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  1,  18, 229,  15, 230, 231,  11,  50,  51,  42,  18,   9,  48,  88,\n",
       "            4,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]]),\n",
       " 'label_len': tensor([10, 28, 28, 10, 18, 19, 20, 16]),\n",
       " 'filenames': ['data/raw_videos\\\\--8pSDeC-fg_11-5-rgb_front.mp4',\n",
       "  'data/raw_videos\\\\--7E2sU6zP4_7-5-rgb_front.mp4',\n",
       "  'data/raw_videos\\\\--dANj_01AU_1-5-rgb_front.mp4',\n",
       "  'data/raw_videos\\\\--8pSDeC-fg_5-5-rgb_front.mp4',\n",
       "  'data/raw_videos\\\\--8pSDeC-fg_9-5-rgb_front.mp4',\n",
       "  'data/raw_videos\\\\--8pSDeC-fg_10-5-rgb_front.mp4',\n",
       "  'data/raw_videos\\\\--8pSDeC-fg_6-5-rgb_front.mp4',\n",
       "  'data/raw_videos\\\\--dANj_01AU_10-5-rgb_front.mp4'],\n",
       " 'raw_labels': ['If so you have high self esteem.',\n",
       "  \"And so what's happened with the idea of acanthus leaf, is that it has is taken on all these different creative looks.\",\n",
       "  'Now make sure your client is comfortable in the side position, supporting your head, knee supported, arms are resting in front comfortably.',\n",
       "  \"But fortunately it's very fixable.\",\n",
       "  'Do you see the positive parts of your personality or do you notice the negative?',\n",
       "  'Are you always focused on the positive aspects of your physical self instead of the negative?',\n",
       "  'When one has low self esteem, the way you can tell is look in the mirror.',\n",
       "  \"I lift it up enough to make sure that I'm comfortable.\"]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Testing model",
   "id": "123f8343f7b38bcd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T21:36:06.433702Z",
     "start_time": "2025-12-02T21:36:06.417463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class PoseToTextModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pose_dim: int,          # D: feature dimension of pose per frame\n",
    "        enc_hidden: int,        # encoder GRU hidden size per direction\n",
    "        vocab_size: int,        # |V|\n",
    "        emb_dim: int,           # token embedding dim\n",
    "        pad_id: int,\n",
    "        num_enc_layers: int = 1,\n",
    "        num_dec_layers: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pad_id = pad_id\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Encoder: Bi-GRU over pose sequence\n",
    "        self.encoder = nn.GRU(\n",
    "            input_size=pose_dim,\n",
    "            hidden_size=enc_hidden,\n",
    "            num_layers=num_enc_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        # Decoder embedding\n",
    "        self.emb = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=emb_dim,\n",
    "            padding_idx=pad_id,\n",
    "        )\n",
    "\n",
    "        # Decoder GRU: hidden size = 2 * enc_hidden (concat directions)\n",
    "        self.decoder = nn.GRU(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=2 * enc_hidden,\n",
    "            num_layers=num_dec_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # Final projection to vocab\n",
    "        self.out = nn.Linear(2 * enc_hidden, vocab_size)\n",
    "\n",
    "    def encode(self, pose, pose_len):\n",
    "        \"\"\"\n",
    "        pose: [B, T, D]\n",
    "        pose_len: [B]\n",
    "        Returns: encoder final hidden state [num_layers*2, B, H]\n",
    "        \"\"\"\n",
    "        # Pack for efficient RNN\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            pose,\n",
    "            lengths=pose_len.cpu(),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "        enc_out, h_n = self.encoder(packed)\n",
    "        # h_n: [num_layers*2, B, enc_hidden]\n",
    "        return h_n\n",
    "\n",
    "    def forward(self, pose, pose_len, labels):\n",
    "        \"\"\"\n",
    "        pose:   [B, T, D]\n",
    "        pose_len: [B]\n",
    "        labels: [B, L]  (with <bos> ... <eos> and <pad>)\n",
    "\n",
    "        We use teacher forcing:\n",
    "          decoder inputs: labels[:, :-1]\n",
    "          targets:        labels[:, 1:]\n",
    "        Returns:\n",
    "          logits: [B, L-1, vocab_size]\n",
    "        \"\"\"\n",
    "        B, T, D = pose.shape\n",
    "        B2, L = labels.shape\n",
    "        assert B == B2\n",
    "\n",
    "        # ---- Encode ----\n",
    "        h_n = self.encode(pose, pose_len)  # [num_layers*2, B, enc_hidden]\n",
    "\n",
    "        # Merge directions for final layer into a single initial hidden state\n",
    "        # For simplicity, we only use last layer’s forward/backward\n",
    "        # h_n_last: [2, B, enc_hidden] -> concat -> [1, B, 2*enc_hidden]\n",
    "        num_layers_times_dir, B_enc, H = h_n.shape\n",
    "        assert B_enc == B\n",
    "        h_n_last = h_n[-2:]                 # [2, B, H] (last layer forward/backward)\n",
    "        h0_dec = torch.cat(\n",
    "            [h_n_last[0], h_n_last[1]], dim=-1\n",
    "        ).unsqueeze(0)                      # [1, B, 2H]\n",
    "\n",
    "        # ---- Decode with teacher forcing ----\n",
    "        # decoder input is labels shifted right (all but last token)\n",
    "        dec_inp = labels[:, :-1]            # [B, L-1]\n",
    "        emb = self.emb(dec_inp)             # [B, L-1, emb_dim]\n",
    "\n",
    "        dec_out, _ = self.decoder(emb, h0_dec)  # [B, L-1, 2H]\n",
    "        logits = self.out(dec_out)              # [B, L-1, vocab_size]\n",
    "\n",
    "        return logits\n"
   ],
   "id": "6f9161bc051737c0",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T21:39:22.022451Z",
     "start_time": "2025-12-02T21:39:22.013348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def build_id_to_token(vocab: dict) -> dict:\n",
    "    \"\"\"vocab: {token: id} -> {id: token}\"\"\"\n",
    "    return {idx: tok for tok, idx in vocab.items()}\n",
    "\n",
    "\n",
    "def tokens_to_text(\n",
    "    ids,\n",
    "    id_to_token,\n",
    "    pad_id: int,\n",
    "    bos_token: str = \"<bos>\",\n",
    "    eos_token: str = \"<eos>\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert a sequence of token IDs into a space-separated string.\n",
    "    Ignores <pad>, optionally removes <bos>, and stops at <eos>.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for i in ids:\n",
    "        if int(i) == pad_id:\n",
    "            continue\n",
    "        tok = id_to_token.get(int(i), \"<unk>\")\n",
    "        if tok == bos_token:\n",
    "            continue\n",
    "        if tok == eos_token:\n",
    "            break\n",
    "        tokens.append(tok)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def bleu1(pred_tokens, ref_tokens):\n",
    "    \"\"\"\n",
    "    Simple BLEU-1 (unigram BLEU) with brevity penalty.\n",
    "    pred_tokens, ref_tokens: lists of tokens (strings).\n",
    "    \"\"\"\n",
    "    if len(pred_tokens) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    pred_counts = Counter(pred_tokens)\n",
    "    ref_counts = Counter(ref_tokens)\n",
    "    overlap = sum(min(pred_counts[w], ref_counts[w]) for w in pred_counts)\n",
    "\n",
    "    precision = overlap / len(pred_tokens)\n",
    "\n",
    "    # brevity penalty\n",
    "    ref_len = len(ref_tokens)\n",
    "    pred_len = len(pred_tokens)\n",
    "    if pred_len == 0:\n",
    "        return 0.0\n",
    "    if pred_len > ref_len:\n",
    "        bp = 1.0\n",
    "    else:\n",
    "        bp = math.exp(1.0 - ref_len / pred_len)\n",
    "\n",
    "    return bp * precision\n",
    "\n",
    "\n",
    "def rouge1_f1(pred_tokens, ref_tokens):\n",
    "    \"\"\"\n",
    "    Very simple ROUGE-1 F1 (over unigrams).\n",
    "    \"\"\"\n",
    "    if not pred_tokens or not ref_tokens:\n",
    "        return 0.0\n",
    "\n",
    "    pred_counts = Counter(pred_tokens)\n",
    "    ref_counts = Counter(ref_tokens)\n",
    "\n",
    "    overlap = sum(min(pred_counts[w], ref_counts[w]) for w in pred_counts)\n",
    "\n",
    "    precision = overlap / len(pred_tokens)\n",
    "    recall = overlap / len(ref_tokens)\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return 2 * precision * recall / (precision + recall)\n"
   ],
   "id": "426a8bbaf6dac421",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T21:39:36.789028Z",
     "start_time": "2025-12-02T21:39:36.769134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pose_dim = batch[\"pose\"].shape[-1]\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "model = PoseToTextModel(\n",
    "    pose_dim=pose_dim,\n",
    "    enc_hidden=256,\n",
    "    vocab_size=vocab_size,\n",
    "    emb_dim=256,\n",
    "    pad_id=my_pad_id,\n",
    ").to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=my_pad_id)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "id_to_token = build_id_to_token(vocab)\n",
    "\n",
    "# if you used these special tokens when building vocab:\n",
    "bos_id = vocab.get(\"<bos>\", None)\n",
    "eos_id = vocab.get(\"<eos>\", None)\n"
   ],
   "id": "38366dd300ae23c9",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T21:41:40.942589Z",
     "start_time": "2025-12-02T21:39:45.956663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    total_bleu = 0.0\n",
    "    total_rouge = 0.0\n",
    "    total_sentences = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        pose = batch[\"pose\"].to(device)          # [B, T, D]\n",
    "        pose_len = batch[\"pose_len\"].to(device)  # [B]\n",
    "        labels = batch[\"labels\"].to(device)      # [B, L]\n",
    "\n",
    "        # ----- forward -----\n",
    "        logits = model(pose, pose_len, labels)   # [B, L-1, V]\n",
    "\n",
    "        # Targets are labels shifted left\n",
    "        target = labels[:, 1:]                   # [B, L-1]\n",
    "\n",
    "        B, Lm1, V = logits.shape\n",
    "        loss = loss_fn(\n",
    "            logits.reshape(B * Lm1, V),\n",
    "            target.reshape(B * Lm1),\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # ----- accumulate loss (per non-pad token) -----\n",
    "        with torch.no_grad():\n",
    "            non_pad = (target != my_pad_id).sum().item()\n",
    "            non_pad = max(non_pad, 1)\n",
    "            total_loss += loss.item() * non_pad\n",
    "            total_tokens += non_pad\n",
    "\n",
    "            # ----- compute BLEU / ROUGE for this batch -----\n",
    "            # Greedy predictions: argmax over vocab\n",
    "            pred_ids_batch = logits.argmax(dim=-1)   # [B, L-1]\n",
    "            ref_ids_batch = target                   # [B, L-1]\n",
    "\n",
    "            for b in range(B):\n",
    "                pred_ids = pred_ids_batch[b].tolist()\n",
    "                ref_ids = ref_ids_batch[b].tolist()\n",
    "\n",
    "                # Convert id sequences to token sequences (strings)\n",
    "                pred_str = tokens_to_text(\n",
    "                    pred_ids,\n",
    "                    id_to_token,\n",
    "                    pad_id=my_pad_id,\n",
    "                    bos_token=\"<bos>\",\n",
    "                    eos_token=\"<eos>\",\n",
    "                )\n",
    "                ref_str = tokens_to_text(\n",
    "                    ref_ids,\n",
    "                    id_to_token,\n",
    "                    pad_id=my_pad_id,\n",
    "                    bos_token=\"<bos>\",\n",
    "                    eos_token=\"<eos>\",\n",
    "                )\n",
    "\n",
    "                pred_toks = pred_str.split()\n",
    "                ref_toks = ref_str.split()\n",
    "\n",
    "                if len(ref_toks) == 0:\n",
    "                    continue  # skip entirely empty reference\n",
    "\n",
    "                b_bleu = bleu1(pred_toks, ref_toks)\n",
    "                b_rouge = rouge1_f1(pred_toks, ref_toks)\n",
    "\n",
    "                total_bleu += b_bleu\n",
    "                total_rouge += b_rouge\n",
    "                total_sentences += 1\n",
    "\n",
    "    avg_loss = total_loss / max(total_tokens, 1)\n",
    "    avg_bleu = total_bleu / max(total_sentences, 1)\n",
    "    avg_rouge = total_rouge / max(total_sentences, 1)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch} | \"\n",
    "        f\"avg token loss: {avg_loss:.4f} | \"\n",
    "        f\"BLEU-1: {avg_bleu:.4f} | ROUGE-1 F1: {avg_rouge:.4f}\"\n",
    "    )\n"
   ],
   "id": "b67b2c8ccab75e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | avg token loss: 5.3558 | BLEU-1: 0.0561 | ROUGE-1 F1: 0.0740\n",
      "Epoch 2 | avg token loss: 4.7113 | BLEU-1: 0.1413 | ROUGE-1 F1: 0.2191\n",
      "Epoch 3 | avg token loss: 4.1272 | BLEU-1: 0.1299 | ROUGE-1 F1: 0.1901\n",
      "Epoch 4 | avg token loss: 3.6539 | BLEU-1: 0.2273 | ROUGE-1 F1: 0.2987\n",
      "Epoch 5 | avg token loss: 3.2051 | BLEU-1: 0.3029 | ROUGE-1 F1: 0.3381\n",
      "Epoch 6 | avg token loss: 2.8048 | BLEU-1: 0.3726 | ROUGE-1 F1: 0.3973\n",
      "Epoch 7 | avg token loss: 2.4183 | BLEU-1: 0.4627 | ROUGE-1 F1: 0.4767\n",
      "Epoch 8 | avg token loss: 2.0040 | BLEU-1: 0.5471 | ROUGE-1 F1: 0.5625\n",
      "Epoch 9 | avg token loss: 1.6619 | BLEU-1: 0.6141 | ROUGE-1 F1: 0.6295\n",
      "Epoch 10 | avg token loss: 1.3469 | BLEU-1: 0.7407 | ROUGE-1 F1: 0.7407\n",
      "Epoch 11 | avg token loss: 1.0729 | BLEU-1: 0.7973 | ROUGE-1 F1: 0.7973\n",
      "Epoch 12 | avg token loss: 0.8664 | BLEU-1: 0.8447 | ROUGE-1 F1: 0.8447\n",
      "Epoch 13 | avg token loss: 0.6858 | BLEU-1: 0.8784 | ROUGE-1 F1: 0.8784\n",
      "Epoch 14 | avg token loss: 0.5726 | BLEU-1: 0.8807 | ROUGE-1 F1: 0.8807\n",
      "Epoch 15 | avg token loss: 0.4827 | BLEU-1: 0.8883 | ROUGE-1 F1: 0.8883\n",
      "Epoch 16 | avg token loss: 0.4110 | BLEU-1: 0.8939 | ROUGE-1 F1: 0.8939\n",
      "Epoch 17 | avg token loss: 0.3609 | BLEU-1: 0.8992 | ROUGE-1 F1: 0.8992\n",
      "Epoch 18 | avg token loss: 0.3233 | BLEU-1: 0.9056 | ROUGE-1 F1: 0.9056\n",
      "Epoch 19 | avg token loss: 0.2922 | BLEU-1: 0.9078 | ROUGE-1 F1: 0.9078\n",
      "Epoch 20 | avg token loss: 0.2677 | BLEU-1: 0.9149 | ROUGE-1 F1: 0.9149\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6a8d3a817ebe30ee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
